{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Priyanshu\\Desktop\\Current Web Projects\\EpicEnigma\\EE_DeepLearing\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 files belonging to 1 classes.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No images found in directory img_align_celeba. Allowed formats: ('.bmp', '.gif', '.jpeg', '.jpg', '.png')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_align_celeba\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Get all images from dir\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_dataset_from_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrgb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Normalize the data\u001b[39;00m\n\u001b[0;32m     10\u001b[0m dataset\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: x \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255.00\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Priyanshu\\Desktop\\Current Web Projects\\EpicEnigma\\EE_DeepLearing\\venv\\Lib\\site-packages\\keras\\src\\utils\\image_dataset.py:303\u001b[0m, in \u001b[0;36mimage_dataset_from_directory\u001b[1;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, **kwargs)\u001b[0m\n\u001b[0;32m    299\u001b[0m image_paths, labels \u001b[38;5;241m=\u001b[39m dataset_utils\u001b[38;5;241m.\u001b[39mget_training_or_validation_split(\n\u001b[0;32m    300\u001b[0m     image_paths, labels, validation_split, subset\n\u001b[0;32m    301\u001b[0m )\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m image_paths:\n\u001b[1;32m--> 303\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    304\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo images found in directory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAllowed formats: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mALLOWLIST_FORMATS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m     )\n\u001b[0;32m    308\u001b[0m dataset \u001b[38;5;241m=\u001b[39m paths_and_labels_to_dataset(\n\u001b[0;32m    309\u001b[0m     image_paths\u001b[38;5;241m=\u001b[39mimage_paths,\n\u001b[0;32m    310\u001b[0m     image_size\u001b[38;5;241m=\u001b[39mimage_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    316\u001b[0m     crop_to_aspect_ratio\u001b[38;5;241m=\u001b[39mcrop_to_aspect_ratio,\n\u001b[0;32m    317\u001b[0m )\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: No images found in directory img_align_celeba. Allowed formats: ('.bmp', '.gif', '.jpeg', '.jpg', '.png')"
     ]
    }
   ],
   "source": [
    "directory = 'img_align_celeba'\n",
    "\n",
    "# Get all images from dir\n",
    "dataset = tf.keras.preprocessing.image_dataset_from_directory(directory=directory, color_mode='rgb',\n",
    "                                                   batch_size=128, image_size=(32,32), label_mode=None,\n",
    "                                                   shuffle=True, seed=42)\n",
    "\n",
    "# Normalize the data\n",
    "\n",
    "dataset=dataset.map(lambda x: x /255.00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Grid of image\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmpl_toolkits\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maxes_grid1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageGrid\n\u001b[1;32m----> 4\u001b[0m it\u001b[38;5;241m=\u001b[39m\u001b[38;5;28miter\u001b[39m(\u001b[43mdataset\u001b[49m)\n\u001b[0;32m      5\u001b[0m one_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(it)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m      6\u001b[0m images \u001b[38;5;241m=\u001b[39m one_batch[:\u001b[38;5;241m16\u001b[39m,:,:,:]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Grid of image\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "it=iter(dataset)\n",
    "one_batch = next(it).numpy()\n",
    "images = one_batch[:16,:,:,:]\n",
    "\n",
    "# show the image\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "grid = ImageGrid(fig, 111, nrows_ncols=(4,4), \n",
    "         axes_pad=0)\n",
    "for ax,im in zip(grid, images):\n",
    "    ax.imshow(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, LeakyReLU,BatchNormalization, Dropout, Flatten, Dense, Activation, Reshape\n",
    "from tensorflow.keras import Model, Input\n",
    "\n",
    "def discriminator_model():\n",
    "    disc_input = Input(shape=(32,32,3), name='discriminator_network')\n",
    "    x=Conv2D(filters=64, kernel_size=3, strides=(2,2), padding='same')(disc_input)\n",
    "    x=LeakyReLU()(x)\n",
    "    x=Dropout(0.2)(x)\n",
    "    \n",
    "    x=Conv2D(filters=128, kernel_size=3, strides=(2,2), padding='same')(x)\n",
    "    x=LeakyReLU()(x)\n",
    "    x=Dropout(0.2)(x)\n",
    "    \n",
    "    x=Conv2D(filters=128, kernel_size=3, strides=(2,2), padding='same')(x)\n",
    "    x=LeakyReLU()(x)\n",
    "    x=Dropout(0.2)(x)\n",
    "    \n",
    "    x=Conv2D(filters=64, kernel_size=3, strides=(2,2), padding='same')(x)\n",
    "    x=LeakyReLU()(x)\n",
    "    x=Dropout(0.2)(x)\n",
    "    \n",
    "    x=Flatten()(x)\n",
    "    x=Dense(1)(x)\n",
    "    \n",
    "    output = Activation('sigmoid')(x)\n",
    "    \n",
    "    disc=Model(inputs = disc_input, outputs=output)\n",
    "    \n",
    "    return disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Priyanshu\\Desktop\\Current Web Projects\\EpicEnigma\\EE_DeepLearing\\venv\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " discriminator_network (Inp  [(None, 32, 32, 3)]       0         \n",
      " utLayer)                                                        \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 16, 16, 64)        1792      \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 4, 4, 128)         147584    \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 2, 2, 64)          73792     \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 2, 2, 64)          0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 2, 2, 64)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 257       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 297281 (1.13 MB)\n",
      "Trainable params: 297281 (1.13 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "disc_model = discriminator_model()\n",
    "disc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Priyanshu\\Desktop\\Current Web Projects\\EpicEnigma\\EE_DeepLearing\\venv\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " generator_network (InputLa  [(None, 100)]             0         \n",
      " yer)                                                            \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 192)               19392     \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 8, 8, 3)           0         \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 8, 8, 3)           12        \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 8, 8, 3)           0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTr  (None, 8, 8, 512)         14336     \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 8, 8, 512)         2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 8, 8, 512)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2D  (None, 16, 16, 256)       1179904   \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 16, 16, 256)       1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 16, 16, 256)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2D  (None, 16, 16, 128)       295040    \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 16, 16, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_3 (Conv2D  (None, 32, 32, 128)       147584    \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 32, 32, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_8 (LeakyReLU)   (None, 32, 32, 128)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_4 (Conv2D  (None, 32, 32, 64)        73792     \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 32, 32, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_9 (LeakyReLU)   (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " conv2d_transpose_5 (Conv2D  (None, 32, 32, 3)         1731      \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " leaky_re_lu_10 (LeakyReLU)  (None, 32, 32, 3)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1736143 (6.62 MB)\n",
      "Trainable params: 1733961 (6.61 MB)\n",
      "Non-trainable params: 2182 (8.52 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv2D, LeakyReLU,BatchNormalization, Dropout, Flatten, Dense, Activation, Reshape, Conv2DTranspose\n",
    "from tensorflow.keras import Model, Input\n",
    "def generator_model(z_dim=100):\n",
    "    gen_input = Input(shape=(z_dim), name='generator_network')\n",
    "    \n",
    "    x = Dense(8*8*3)(gen_input)\n",
    "    x = Reshape(target_shape=(8,8,3))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    \n",
    "    x = Conv2DTranspose(filters=512, kernel_size=3, strides=(1,1), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    \n",
    "    x = Conv2DTranspose(filters=256, kernel_size=3, strides=(2,2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    \n",
    "    x = Conv2DTranspose(filters=128, kernel_size=3, strides=(1,1), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    \n",
    "    x = Conv2DTranspose(filters=128, kernel_size=3, strides=(2,2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    \n",
    "        \n",
    "    x = Conv2DTranspose(filters=64, kernel_size=3, strides=(1,1), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    \n",
    "    x = Conv2DTranspose(filters=3, kernel_size=3, strides=(1,1), padding='same')(x)\n",
    "    fake_images_gen = LeakyReLU()(x)\n",
    "    \n",
    "    model_gen=Model(inputs = gen_input, outputs=fake_images_gen)\n",
    "    \n",
    "    return model_gen\n",
    "gen_model = generator_model()\n",
    "gen_model.summary(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Priyanshu\\Desktop\\Current Web Projects\\EpicEnigma\\EE_DeepLearing\\venv\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " model_input (InputLayer)    [(None, 100)]             0         \n",
      "                                                                 \n",
      " model_3 (Functional)        (None, 32, 32, 3)         1736143   \n",
      "                                                                 \n",
      " model_2 (Functional)        (None, 1)                 297281    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2033424 (7.76 MB)\n",
      "Trainable params: 1733961 (6.61 MB)\n",
      "Non-trainable params: 299463 (1.14 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "disc_model = discriminator_model()\n",
    "gen_model = generator_model()\n",
    "# Image Classification CNN\n",
    "# Compile the model that trains disc.\n",
    "disc_model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# Compile the model that trains generator\n",
    "z_dim = 100\n",
    "disc_model.trainable = False\n",
    "model_input = Input(shape=(z_dim), name='model_input')\n",
    "model_output = disc_model(gen_model(model_input))\n",
    "\n",
    "model = Model(model_input, model_output)\n",
    "\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(batch, batch_size):\n",
    "    valid = np.ones((batch_size, 1)) \n",
    "    fake = np.zeros((batch_size, 1)) \n",
    "    \n",
    "    disc_model.train_on_batch(batch, valid)\n",
    "    \n",
    "    noise = np.random.normal(0,1, (batch_size, z_dim))\n",
    "    gen_image = gen_model.predict(noise)\n",
    "    disc_model.train_on_batch(gen_image, fake)\n",
    "def train_generator(batch_size):\n",
    "    valid = np.ones((batch_size, 1)) \n",
    "    noise = np.random.normal(0,1, (batch_size, z_dim))\n",
    "    model.train_on_batch(noise, valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(fake_image, path):\n",
    "    image = tf.keras.preprocessing.image.array_to_img(fake_image.numpy())\n",
    "    tf.keras.preprocessing.image.save_img(path, image)\n",
    "    return image\n",
    "def generate_and_save_image(path):\n",
    "    noise = np.random.normal(0,1, (1, z_dim))\n",
    "    fake_images = gen_model(noise)\n",
    "    image = save_image(fake_images[0], path)\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epochs in range(10):\n",
    "    print(f'No of Epochs-> {epochs}')\n",
    "    for i,batch in enumerate(dataset):\n",
    "        train_discriminator(batch, batch.shape[0])\n",
    "        train_generator(batch.shape[0])\n",
    "        if i%100==0:\n",
    "            print(f'Batch Images-> {i}')\n",
    "            PATH = 'generate_face'\n",
    "            generate_and_save_image(path = f'{PATH}/epochs_{epochs}batch_{i}.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
